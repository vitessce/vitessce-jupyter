from pathlib import Path, PosixPath
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider

S3 = S3RemoteProvider()

BUCKET_VERSION = "0.0.32"
BUCKET_NAME = "vitessce-data"
BUCKET = PosixPath(BUCKET_NAME) / BUCKET_VERSION / "main"

include: "./common.smk"

# All subworkflows listed here will be
# executed as part of the 'all' rule
# in this snakefile.
subworkflow habib_2017_nature_methods:
    workdir:
        "habib_2017_nature_methods"

subworkflow human_lymph_node_10x_visium:
    workdir:
        "human_lymph_node_10x_visium"


# Only access workflow._subworkflows after all
# subworkflows have been defined above.
SUBWORKFLOW_KEYS = workflow._subworkflows.keys()

rule all:
    input:
        in_files=[
            # TODO: use config.yml .output here
            globals()[subworkflow_key](str(PROCESSED_DIR / f"{subworkflow_key}.h5ad.zarr"))
            for subworkflow_key
            in SUBWORKFLOW_KEYS
        ]
    params:
        should_upload=config.get("upload"),
        # This should be in output: https://github.com/snakemake/snakemake/issues/870
        out_files=[
            S3.remote(str(BUCKET / subworkflow_key / f"{subworkflow_key}.h5ad.zarr"))
            for subworkflow_key
            in SUBWORKFLOW_KEYS
        ]
    run:
        if str2bool(params.should_upload):
            print("Uploading")
            for in_file, out_file in zip(input.in_files, params.out_files):
                shell(f"aws s3 cp --recursive {in_file} s3://{out_file}")
        else:
            print("Not uploading. To upload, use snakemake --config upload=true")
            for in_file, out_file in zip(input.in_files, params.out_files):
                shell(f"echo \"{in_file} s3://{out_file}\"")
